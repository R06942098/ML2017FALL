{\rtf1\ansi\ansicpg950\cocoartf1561\cocoasubrtf100
{\fonttbl\f0\fmodern\fcharset0 Courier;\f1\fnil\fcharset136 PingFangTC-Regular;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\paperw11900\paperh16840\margl1440\margr1440\vieww15580\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\partightenfactor0

\f0\fs26 \cf0 \expnd0\expndtw0\kerning0
# ASR+Translation\
\
Team : ntu_r06942112_final\
Team members :\
\
```\
r06942098 
\f1 \'b4\'bf\'ac\'66\'b0\'b6 	r06942112 \'b1\'69\'b9\'c5\'c5\'ef\

\f0 r06943124 
\f1 \'a4\'fd\'e0\'b1\'b3\'cd	r06943121 \'bf\'bd\'aa\'da\'a9\'76\
```
\f0 \
\
## Used package\
Sklearn : from sklearn.model_selection import train_test_split SampleSubmission.csv\
jieba : import jieba\
	  jieba.set_dictionary(\'91dict.txt.big\'92)\
\
gensim_Word2Vec : from gensim.models import Word2Vec\
\
##compile \
```\
bash final.sh train.data train.caption test.data test.csv\
```\
\
\
\
## Model:\
```\
Retrieval_based + attention\
```\
\
\
\
## diagram\
```\
 (1) pad zero to the post of training/testing data\
 (2) siamese network : Mfcc -> no embedding layers\
					Caption -> pad_sequences -> embedding_layers\
 (3) attention mechanism : dot(Mfcc, word vector of caption)\
 (4) cosine similarity : dot(Mfcc,attention mechanism[flatten])\
 (5) hinge loss margin : 0.2\
 (6) simultaneously train contrastive and prediction model \
```\
## keep in mind in the future\
\
 (1) regularizer term : dropout / l2 / l2\
 (2) output is a score : dummy variable , two models(prediction/contrastive) train simultaneously\
 (3)Word2Vec in Chinese may not get the good word vector.when training our model, embedding layer\'92s trainable -> True\
\
}